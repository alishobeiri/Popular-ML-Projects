| Title                                                                                                                                                                                                                                                                                              | URL                                            |   Score | Date                |
|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------|--------:|:--------------------|
| [R] Researchers discover that in-context learning creates task vectors in LLMs                                                                                                                                                                                                                     | https://arxiv.org/pdf/2310.15916.pdf           |     149 | 2023-10-25 20:50:38 |
| [R] QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models - Institute of Science and Technology Austria (ISTA) 2023 - Can compress the 1.6 trillion parameter SwitchTransformer-c2048 model to less than 160GB (20x compression, 0.8 bits per parameter) at only minor accuracy loss! | https://arxiv.org/abs/2310.16795               |      42 | 2023-10-26 19:01:24 |
|                                                                                                                                                                                                                                                                                                    | https://github.com/ist-daslab/qmoe             |         |                     |
| [D] LLMs playing chess are sensitive to how the position came to be                                                                                                                                                                                                                                | https://github.com/dpaleka/llm-chess-proofgame |      38 | 2023-10-25 23:30:47 |
| [D][R] How should the architecture of a transformer be scaled?                                                                                                                                                                                                                                     | https://arxiv.org/abs/2302.13971)              |      29 | 2023-10-25 10:19:09 |
|                                                                                                                                                                                                                                                                                                    | https://arxiv.org/abs/2001.08361               |         |                     |
|                                                                                                                                                                                                                                                                                                    | https://arxiv.org/abs/2109.10686)              |         |                     |