| Title                                                                                                                          | URL                                                                            |   Score | Date                |
|:-------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------|--------:|:--------------------|
| [D] Potential Plagiarism in ICLR 2024 Spotlight: Shengjie Luo and Tianlang Chen's "Gaunt Tensor Products"                      | https://github.com/lsj2408/Gaunt-Tensor-Product/issues/2                       |     258 | 2024-10-21 14:52:17 |
| [R] Google Shopping 10M dataset for large scale multimodal product retrieval and ranking                                       | https://arxiv.org/abs/2404.08535                                               |     149 | 2024-10-20 21:51:41 |
|                                                                                                                                | https://github.com/marqo-ai/GCL                                                |         |                     |
| [R] RWKV-7: attention-free and surpassing strong Modded-GPT baseline (the one with Muon optimizer), while only using headsz 64 | https://github.com/BlinkDL/modded-nanogpt-rwkv                                 |      98 | 2024-10-21 14:18:19 |
| [R] Gradient accumulation bug fix in nightly transformers                                                                      | https://github.com/unslothai/unsloth/                                          |      57 | 2024-10-21 19:37:38 |
|                                                                                                                                | https://github.com/huggingface/trl/issues/2175                                 |         |                     |
|                                                                                                                                | https://github.com/huggingface/transformers/issues/14638                       |         |                     |
|                                                                                                                                | https://github.com/huggingface/transformers/pull/34191#issuecomment-2418658361 |         |                     |
| [R] How do RoPE-based LLMs learn attention sinks (or encode absolute positions)?                                               | https://arxiv.org/pdf/2309.17453)                                              |      40 | 2024-10-21 19:46:28 |