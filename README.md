| Title                                                                                                                                              | URL                                          |   Score | Date                |
|:---------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------|--------:|:--------------------|
| [R] Why is AdamW often superior to Adam with L2-Regularization in practice? The answer may lie in how weight decay balances updates across layers. | https://arxiv.org/abs/2305.17212             |      83 | 2023-10-08 15:17:39 |
| [R] Why do we need weight decay in modern deep learning? ðŸ¤”                                                                                         | https://arxiv.org/abs/2310.04415             |      54 | 2023-10-09 16:20:51 |
|                                                                                                                                                    | https://github.com/tml-epfl/why-weight-decay |         |                     |