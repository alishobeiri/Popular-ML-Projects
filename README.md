| Title                                                                                                                                            | URL                                  |   Score | Date                |
|:-------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------|--------:|:--------------------|
| [R] MIT, Meta, CMU Researchers: LLMs trained with a finite attention window can be extended to infinite sequence lengths without any fine-tuning | https://arxiv.org/pdf/2309.17453.pdf |     255 | 2023-10-03 12:56:26 |
| [R] Think before you speak: Training Language Models With Pause Tokens                                                                           | https://arxiv.org/abs/2310.02226     |      64 | 2023-10-04 12:59:23 |