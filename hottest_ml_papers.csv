Title,URL,Score,Date
[R] RedPajama-Data-v2: an Open Dataset with 30 Trillion Tokens for Training Large Language Models,https://github.com/togethercomputer/RedPajama-Data,55,2023-10-30 21:55:00
[R] Announcing Distil-Whisper - 6x faster than Whisper-large-v2 and performs within 1% WER on out-of-distribution,"https://github.com/huggingface/distil-whisper/tree/main
https://github.com/huggingface/distil-whisper/blob/main/Distil_Whisper.pdf",31,2023-10-31 17:49:08
