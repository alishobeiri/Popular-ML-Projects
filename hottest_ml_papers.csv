Title,URL,Score,Date
[D] The ML Papers That Rocked Our World (2020-2023),"https://arxiv.org/abs/2210.05189
https://arxiv.org/abs/2212.13345
https://arxiv.org/abs/2106.09685
https://arxiv.org/abs/2201.02177
https://arxiv.org/abs/2010.11929
https://arxiv.org/abs/2104.14294
https://arxiv.org/abs/2012.12877v2
https://arxiv.org/abs/2103.14030
https://arxiv.org/abs/2201.03545
https://arxiv.org/abs/2103.00020
https://arxiv.org/abs/2112.10752
https://arxiv.org/abs/2006.11239
https://arxiv.org/abs/2207.12598
https://arxiv.org/abs/2012.09841
https://arxiv.org/abs/2304.02643
https://arxiv.org/abs/2304.07193
https://arxiv.org/abs/2308.07037
https://arxiv.org/abs/2005.14165
https://arxiv.org/abs/2201.11903
https://arxiv.org/abs/2203.02155
https://arxiv.org/abs/2203.15556
https://arxiv.org/abs/2301.13688
https://arxiv.org/abs/2302.13971
https://arxiv.org/abs/2302.04761
https://arxiv.org/abs/2003.08934
https://github.com/dmarx/anthology-of-modern-ml",272,2023-09-14 13:50:27
[P] Llama2 inference in a single file of pure Mojo,"https://github.com/tairov/llama2.mojo
https://github.com/tairov/llama2.py
https://github.com/karpathy/llama2.c",94,2023-09-14 12:32:42
"[R] Uncovering mesa-optimization algorithms in Transformers (from Google Research, ETH ZÃ¼rich, and Google DeepMind)",https://arxiv.org/abs/2309.05858,48,2023-09-15 12:31:13
