Title,URL,Score,Date
"[R] QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models - Institute of Science and Technology Austria (ISTA) 2023 - Can compress the 1.6 trillion parameter SwitchTransformer-c2048 model to less than 160GB (20x compression, 0.8 bits per parameter) at only minor accuracy loss!","https://arxiv.org/abs/2310.16795
https://github.com/ist-daslab/qmoe",103,2023-10-26 19:01:24
[R] What Algorithms can Transformers Learn? A Study in Length Generalization,https://arxiv.org/abs/2310.16028,31,2023-10-26 19:13:38
